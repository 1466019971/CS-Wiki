# 📊 表格型方法 Tabular Methods

---

本节课我们通过最简单的 **表格型的方法(tabular methods)** 来讲解如何使用 `value-based` 方法去求解强化学习问题。

## 1. Q-Table

我们可以用状态动作价值 `action-value` 来表达说在某个状态下，为什么动作 1 会比动作 2 好，因为动作 1 的价值比动作 2 要高，这个状态动作价值函数就叫 `Q 函数`。$Q^\pi(s_t,a_t)$ 表示从状态 $s_t$ 出发，已经采取了动作 $a_t$ 之后，再使用策略 π（概率分布）所带来的累积奖赏（MDP 章节我们介绍过）

**如果 `Q 表格`是一张已经训练好的表格的话，那这一张表格就像是一本生活手册。**我们就知道在熊发怒的时候，装死的价值会高一点。在熊离开的时候，我们可能偷偷逃跑的会比较容易获救。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025112152.png" style="zoom: 60%;" />

这张表格里面 Q 函数的意义就是在这个状态下，选择了这个动作，后续能够一共拿到多少【总收益】。**所以强化学习的目标导向性很强，环境给出的 reward 是一个非常重要的反馈，它就是根据环境的 reward 来去做选择**，我选择某个动作是因为我未来可以拿到的 reward 会更高一点。

❓ **为什么用未来的总收益来评价当前这个动作是好是坏？**

举个例子，假设一辆车在路上，当前是红灯，我们直接走的收益就很低，因为违反交通规则，这就是当前的单步收益。可是如果我们这是一辆救护车，我们正在运送病人，把病人快速送达医院的收益非常的高，而且越快你的收益越大。在这种情况下，我们很可能应该要闯红灯，因为未来的远期收益太高了。这也是为什么强化学习需要去学习远期的收益，因为在现实世界中奖励往往是延迟的，是有 delay 的。所以我们一般会从当前状态开始，把后续有可能会收到所有收益加起来计算当前动作的 Q 的价值，让 Q 的价值可以真正地代表当前这个状态下，动作的真正的价值。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025112639.png" style="zoom: 60%;" />

但有的时候你把目光放得太长远不好，因为如果事情很快就结束的话，你考虑到最后一步的收益无可厚非。如果是一个持续的没有尽头的任务，即<u>**持续式任务(Continuing Task)**，你把未来的收益全部相加，作为当前的状态价值就很不合理。股票的例子就很典型了</u>，我们要关注的是累积的收益。可是如果说十年之后才有一次大涨大跌，你显然不会把十年后的收益也作为当前动作的考虑因素。**这就是 MDP 引入衰减因子 $\gamma$ 的原因**，γ∈[0,1]，越往后 $\gamma^n$ 就会越小，也就是说越后面的收益对当前价值的影响就会越小。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025112856.png" style="zoom:60%;" />

类似于下图，**最后我们要求解的就是一张 Q 表格**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201025120046.png" style="zoom:40%;" />

- 它的行数是所有的状态数量，一般可以用坐标来表示表示格子的状态，也可以用 1、2、3、4、5、6、7 来表示不同的位置。
- Q 表格的列表示不同的动作。

最开始这张 Q 表格会全部初始化为零，然后 agent 会不断地去和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个行动的平均总收益去更新这个 Q 表格。怎么去更新 Q 表格就是接下来要引入的【强化】概念：

- **`强化`就是我们可以用下一个状态的价值来更新当前状态的价值**。在强化学习里面，你可以每走一步更新一下 Q 表格，然后用下一个状态的 Q 值来更新这个状态的 Q 值，这种**单步更新**的方法叫做 `时序差分` 👇。

## 2. 时序差分 Temporal Difference

时序差分（Temporal Difference,TD）：

**为了让大家更好地理解时序差分这种更新方法，这边给出它的物理意义。**我们先理解一下巴普洛夫的条件反射实验：

<img src="https://gitee.com/veal98/images/raw/master/img/20201025120634.png" style="zoom: 55%;" />

这个实验讲的是小狗会对盆里面的食物无条件产生刺激，分泌唾液。一开始小狗对于铃声这种中性刺激是没有反应的，可是我们把这个铃声和食物结合起来，每次先给它响一下铃，再给它喂食物，多次重复之后，当铃声响起的时候，小狗也会开始流口水。**盆里的肉可以认为是强化学习里面那个延迟的 reward，声音的刺激可以认为是有 reward 的那个状态之前的一个状态**。多次重复实验之后，最后的这个 reward 会强化小狗对于这个声音的条件反射，它会让小狗知道这个声音代表着有食物，这个声音对于小狗来说也就有了价值，它听到这个声音也会流口水。

巴普洛夫效应揭示的是中性刺激(铃声)跟无条件刺激(食物)紧紧挨着反复出现的时候，条件刺激也可以引起无条件刺激引起的唾液分泌，然后形成这个条件刺激。**这种中性刺激跟无条件刺激在时间上面的结合，我们就称之为强化。** 强化的次数越多，条件反射就会越巩固。小狗本来不觉得铃声有价值的，经过强化之后，小狗就会慢慢地意识到铃声也是有价值的，它可能带来食物。更重要是一种条件反射巩固之后，我们再用另外一种新的刺激和条件反射去结合，还可以形成第二级条件反射，同样地还可以形成第三级条件反射。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025121148.png" style="zoom:60%;" />

在人的身上是可以建立多级的条件反射的，举个例子，比如说一般我们遇到熊都是这样一个顺序，看到树上有熊爪，然后看到熊之后，突然熊发怒，扑过来了。经历这个过程之后，我们可能最开始看到熊才会瑟瑟发抖，后面就是看到树上有熊爪就已经有害怕的感觉了。也就说在不断的重复试验之后，下一个状态的价值，它是可以不断地去强化影响上一个状态的价值的。

<img src="https://gitee.com/veal98/images/raw/master/img/20201025121524.png" style="zoom:50%;" />

如上图所示，这种强化方式可以用一行公式来表示，这种更新的方式叫做`时序差分(Temporal Difference)`。这个公式就是说拿下一步的 Q 值 $Q(S_{t+_1},A_{t+1})$ 来更新当前这一步的 Q 值 $Q(S_t,A_t)$ 。

为了理解这个公式，如上图所示，我们先把 $R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right.)$ 当作是一个目标值，就是 $Q(S_t,A_t)$ 想要去逼近的一个目标值。我们想要计算的就是 $Q(S_t,A_t)$ 。**因为最开始 Q 值都是随机初始化或者是初始化为零，它需要不断地去逼近它理想中真实的 Q 值，我们就叫 target 。Target 就是带衰减的未来收益的总和。**

我们用 $G_t$ 来表示未来收益总和(return)，并且对它做一下数学变化：

<img src="https://gitee.com/veal98/images/raw/master/img/20201025121752.png" style="zoom:67%;" />

也就是说，我们拿 $Q(S_t,A_t)$ 来逼近 $G_t$，那 $Q(S_{t+1},A_{t+1})$ 其实就是近似 $G_{t+1}$。我就可以用 $Q(S_{t+1},A_{t+1})$近似 $G_{t+1}$，然后把 $R_{t+1}+Q(S_{t+1},A_{t+1})$ 当成目标值。

$Q(S_t,A_t)$ 就是要逼近这个目标值，我们用**软更新**的方式来逼近。软更新的方式就是每次我只更新一点点，α 有点类似于学习率。最终的话，Q 值都是可以慢慢地逼近到真实的 target 值。这样我们的更新公式只需要用到当前时刻的 $S_{t},A_t$，还有拿到的 $R_{t+1}, S_{t+1}，A_{t+1}$ 。

**该算法由于每次更新值函数需要知道当前的状态(state)、当前的动作(action)、奖励(reward)、下一步的状态(state)、下一步的动作(action)，即 ($S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 这几个值 ，由此得名 `Sarsa` 算法**。它走了一步之后，拿到了 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 之后，就可以做一次更新。

## 3. Sarsa: On-policy TD Control

### ① On-policy / Off-policy

强化学习可分为 **在线学习 On-policy** 和 **离线学习 Off-policy**

**On-Policy 在线学习：智能体 agent 本身必须与环境进行互动，然后一边选取动作一边学习**，也可以通过别人的经验进行学习，也就是说经验是共享的，可以是自己的过往经验也可以是其他人的学习经验。

最典型的在线学习就是 Sarsa 了, 还有一种优化 Sarsa 的算法, 叫做 Sarsa lambda, 最典型的离线学习就是 Q learning, 后来人也根据离线学习的属性, 开发了更强大的算法, 比如让计算机学会玩电动的 Deep-Q-Network

🔸 Sarsa 是一种 on-policy 策略。Sarsa 优化的是它实际执行的策略，它直接拿下一步会执行的 action 来去优化 Q 表格，所以 **on-policy 在学习的过程中，只存在一种策略**，它用一种策略去做 action 的选取，也用一种策略去做优化。Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，所以它就会在优化它自己的策略的时候，尽可能的离悬崖远一点。这样就保证下一步哪怕是有随机动作，它也还是在安全区域内。

🔸 **off-policy 在学习的过程中，有两种不同的策略**：

- 第一个策略是我们需要去学习的策略，即`target policy(目标策略)`，一般用 π 来表示，Target policy 就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。

- 另外一个策略是探索环境的策略，即`behavior policy(行为策略)`，一般用 μ 来表示。μ 可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据喂给 target policy 去学习。而且喂给目标策略的数据中并不需要 $A_{t+1}$ ，而 Sarsa 是要有 $A_{t+1}$ 的。**Behavior policy 像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习**。比如目标策略优化的时候，Q-learning 才不管你下一步去往哪里探索，会不会掉进悬崖，我就只选我收益最大一个最优的策略。

  <img src="https://gitee.com/veal98/images/raw/master/img/20201026101850.png" style="zoom:40%;" />

对比一下 On-policy 和 Off-policy：

<img src="https://gitee.com/veal98/images/raw/master/img/20201026110020.png" style="zoom: 40%;" />

### ② Sarsa

<img src="https://gitee.com/veal98/images/raw/master/img/20201026095859.png" style="zoom: 67%;" />

我们直接看这个框框里面的更新公式， 和上面的时序差分单步更新公式是一模一样的。S' 就是 $S_{t+1}$ 。我们就是拿下一步的 Q 值 $Q(S',A')$ 来更新这一步的 Q 值 $Q(S,A)$，不断地强化每一个 Q。

## 4. Q-learning: Off-policy TD Control

Q-learning 的算法有两种 policy：behavior policy 和 target policy。

Target policy π 直接在 Q-table 上取 greedy，就取它下一步能得到的所有状态，如下式所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201026102006.png" style="zoom:40%;" />

Behavior policy μ 可以是一个随机的 policy，但**我们采取 `ε-greedy`，让 behavior policy 不至于是完全随机的，它是基于 Q-table 逐渐改进的**。

`ε-greedy` 的意思是说，我们有 1−ε 的概率会按照 Q-function 来决定 action，通常 ε 就设一个很小的值， 1−ε 可能是 90%，也就是 90% 的概率会按照 Q-function 来决定 action，但是你有 10% 的机率是随机的。通常在实现上 ε  会随着时间递减。在最开始的时候。因为还不知道那个 action 是比较好的，所以你会花比较大的力气在做 exploration。接下来随着 training 的次数越来越多。已经比较确定哪一个 Q 是比较好的。你就会减少你的 exploration，你会把 ε 的值变小，主要根据 Q-function 来决定你的 action，比较少做随机选择，这是 ε-greedy。

**Sarsa 和 Q-learning 的更新公式都是一样的，区别只在目标值 target 计算的这一部分：**

- Sarsa 是 $R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})$
- Q-learning 是 $R_{t+1}+\gamma \underset{a}{\max} Q\left(S_{t+1}, a\right)$

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
- [【强化学习】强化学习分类](https://blog.csdn.net/qq_30615903/article/details/80765106)
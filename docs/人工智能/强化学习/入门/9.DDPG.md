# 🍨 Deep Deterministic Policy Gradient（DDPG）

---

**Actor-Critic 涉及到了两个神经网络, 而且每次都是在连续状态中更新参数, 每次参数更新前后都存在相关性, 导致神经网络只能片面的看待问题, 甚至导致神经网络学不到东西**. 想想我们之前介绍的DQN是如何解决的这个问题的？就是建立了两个网络，一个Q目标网络，一个Q现实网络，同时使用了经验回放机制。那么如果在 `Actor-Critic` 网络结构中加入这两个机制，就得到了一种新的强化学习模型：`Deep Deterministic Policy Gradient`，简称`DDPG`。可以说 **Actor-Critic + DQN = DDPG**。

![](https://gitee.com/veal98/images/raw/master/img/20201102102711.png)

## 1. Deep 和 DQN

Deep 顾名思义, 就是走向更深层次, 我们在 DQN 中提到过, 使用一个记忆库和两套结构相同但参数更新频率不同的神经网络能有效促进学习. 那我们也把这种思想运用到 DDPG 当中, 使 DDPG 也具备这种优良形式. 但是 DDPG 的神经网络形式却比 DQN 的要复杂一点点.

![](https://gitee.com/veal98/images/raw/master/img/20201102102811.png)

## 2. Deterministic Policy Gradient

Policy gradient 相比其他的强化学习方法, 它能被用来在连续动作上进行动作的筛选 . 而且筛选的时候是根据所学习到的动作分布随机进行筛选, 而 Deterministic 改变了输出动作的过程, 斩钉截铁的只在连续动作上输出一个动作值.

![](https://gitee.com/veal98/images/raw/master/img/20201102105726.png)

我们看一下 DDPG 关于此的概念定义：

🔸 **确定性行为策略 `μ`** : 定义为一个函数，每一步的行为可以通过 $a_{t} = \mu(s_{t}) $ 计算获得。

🔸 **策略网络**：用一个卷积神经网络对 μ 函数进行模拟，这个网络我们就叫做策略网络，其参数为 $\theta^{\mu}$

🔸 **behavior policy `β`** : 在 RL 训练过程中，我们要兼顾 2 个 e : exploration 和 exploit：

<u>exploration 的目的是探索潜在的更优策略</u>，所以训练过程中，**我们为 action 的决策机制引入随机噪声，将action 的决策从确定性过程变为一个随机过程， 再从这个随机过程中采样得到确定的 action，下达给环境执行**。过程如下图所示：

![](https://gitee.com/veal98/images/raw/master/img/20201113165016.png)

上述这个策略叫做 behavior 策略，用 β 来表示, 这时 RL 的训练方式叫做 off-policy.

这里与 $\epsilon-greedy$ 的思路是类似的。

DDPG中，使用 [Uhlenbeck-Ornstein随机过程](https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process)（下面简称 `UO` 过程），作为引入的随机噪声。UO过程在时序上具备很好的相关性，可以使 agent 很好的探索具备动量属性的环境。

🚨 **注意**：<u>这个 `β` 不是我们想要得到的最优策略，仅仅在训练过程中，生成下达给环境的 action， 从而获得我们想要的数据集，比如状态转换 (transitions)、或者 agent 的行走路径等，然后利用这个数据集去训练策略 `μ` ，以获得最优策略。在测试 test 和评估 evaluation 时，使用 `μ`，不会再使用 `β`。</u>

## 3. DDPG 神经网络

现在我们来说说 DDPG 中所用到的神经网络. 它其实和我们之前提到的 Actor-Critic 形式差不多, 也需要有基于 策略 Policy 的神经网络（`策略网络`） 和基于 价值 Value 的神经网络（`价值网络 / Q 网络`）, 但是为了体现 DQN 的思想, 每种神经网络我们都需要再细分为两个（`online 网络`和 `target 网络`）：

- **【策略网络】**Policy Gradient 这边, 我们有**估计网络 online**和**现实网络 target**, <u>估计网络用来输出实时的动作, 供 actor 在现实中实行. 而现实网络则是用来更新价值网络系统的</u>. 
- **【Q 网络】**我们再来看看价值系统这边, 我们也有现实网络和估计网络, 他们都在输出这个状态的价值, 而输入端却有不同, 状态现实网络这边会拿着从动作现实网络来的动作加上状态的观测值加以分析, 而状态估计网络则是拿着当时 Actor 施加的动作当做输入.在实际运用中, DDPG 的这种做法的确带来了更有效的学习过程.

![](https://gitee.com/veal98/images/raw/master/img/20201102110538.png)

⭐ 我们采用了类似 DQN 的双网络结构，同样的我们**只需要训练 online 网络的参数，而 target 网络的参数是由前面两个网络每隔一定的时间复制过去的**。

<img src="https://gitee.com/veal98/images/raw/master/img/20201113165957.png" style="zoom: 80%;" />

在训练完一个 mini-batch 的数据之后，通过梯度下降算法更新 online 网络的参数，然后再通过 **soft update 算法**更新 target 网络的参数：

<img src="https://gitee.com/veal98/images/raw/master/img/20201113170145.png" style="zoom: 80%;" />

## 4. DDPG 整体算法详解

![](https://gitee.com/veal98/images/raw/master/img/20201113170644.png)

🚕 初始化 actor / critic 的 online 神经网络参数: $\theta^{Q}$ 和 $\theta^{\mu}$ ； 

🚕 将 online  网 络的参数拷贝给对应的 target 网络参数 ： $\theta^{Q{\prime}} \leftarrow \theta^{Q}$, $\theta^{\mu{\prime}} \leftarrow \theta^{\mu} $ ；

🚕 初始化 replay memory buffer R ;

🚕 for each episode:

- 初始化 UO 随机过程；

- for t = 1, T:

  - actor 根据 behavior 策略选择一个 $a_t$ , 下达给环境执行该动作

    <img src="https://gitee.com/veal98/images/raw/master/img/20201113171214.png" style="zoom:80%;" />

    behavior策略是一个根据当前 online 策略 μ 和随机 UO 噪声生成的随机过程, 从这个随机过程采样获得 $a_{t}$ 的值。

  - 环境执行动作 $a_t$ 并产生新的状态 $s_{t+1}$

  - actor 将这个状态转换过程(transition): $(s_{t}, a_{t}, r_{t}，s_{t+1}) $ 存入replay memory buffer R 中，作为训练 online 网络的数据集

  - 从 replay memory buffer R中，随机采样 N 个 transition 数据，作为 online 策略网络、 online Q 网络的一个 mini-batch 训练数据。我们用$(s_{i}, a_{i}, r_{i}，s_{i+1}) $表示 mini-batch 中的单个 transition 数据

  - 计算 online Q网络的梯度 gradient：
    Q 网络的 loss 定义：使用类似于监督式学习的方法，定义 loss为 MSE: mean squared error：

    <img src="https://gitee.com/veal98/images/raw/master/img/20201113171548.png" style="zoom:80%;" />

    其中， $y_{i}$  可以看做"标签"：

    <img src="https://gitee.com/veal98/images/raw/master/img/20201113171622.png" style="zoom:80%;" />

    基于标准的反向传播方法 back-propagation，就可以求得 L 针对  $\theta^{Q}$ 的梯度 gradient：$ \triangledown_{\theta^{Q}} L $ 。

    有两点值得注意：

    ① $y_{i}$  的计算，使用的是 target 策略网络 μ ′   和 target Q 网络 Q ′  。这样做是为了Q网络参数的学习过程更加稳定，易于收敛。

    ② 这个标签本身依赖于我们正在学习的 target 网络，这是区别于监督式学习的地方。
    
  - update online Q： 采用 Adam optimizer 更新  $\theta^{Q}$ 

  - 计算策略网络的策略梯度 policy gradient：
    

  policy gradient的定义：表示 performance objective 的函数J  针对 $\theta^{\mu}$ 的 gradient。 根据 2015 D.Silver 的[DPG 论文](http://xueshu.baidu.com/s?wd=paperuri:(43a8642b81092513eb6bad1f3f5231e2)&filter=sc_long_sign&sc_ks_para=q=Deterministic policy gradient algorithms&sc_us=6855198342873463498&tn=SE_baiduxueshu_c1gjeupa&ie=utf-8)中的数学推导，在采用off-policy的训练方法时，policy gradient算法如下：

  ![](https://gitee.com/veal98/images/raw/master/img/20201113172144.png)

  根据 Monte-carlo 方法，使用 mini-batch 数据代入上述 policy gradient 公式，可以作为对上述期望值的一个无偏差估计 (un-biased estimate), 所以 policy gradient 可以改写为

    <img src="https://gitee.com/veal98/images/raw/master/img/20201113172035.png" style="zoom: 67%;" />

  - 	update online 策略网络：采用 Adam optimizer更新 $\theta^{\mu}$ 
      
  - 	soft update target 网络 μ ′ 和 Q ′ ：
      <img src="https://gitee.com/veal98/images/raw/master/img/20201113170145.png" style="zoom: 80%;" />
      
  - end for time step

- 🚕 end for episode

![](https://gitee.com/veal98/images/raw/master/img/20201113172444.png)

## 5. 代码实现

> ✅ TODO

## 📚 References

- [莫烦 Python — 强化学习](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL-methods/)
- [Github - DeepRL-TensorFlow2](https://github.com/marload/DeepRL-TensorFlow2) - 🐋 Simple implementations of various popular Deep Reinforcement Learning algorithms using TensorFlow2
- [Deep Reinforcement Learning - 1. DDPG原理和算法](https://blog.csdn.net/kenneth_yu/article/details/78478356)
# 🎤 模仿学习/示教学习 Imitation Learning

---

## 1. 什么是模仿学习

`Imitation learning` 讨论的问题是：**假设我们连 reward 都没有，那要怎么办呢？** 

Imitation learning 又叫做 `learning from demonstration(示范学习)` ，`apprenticeship learning(学徒学习)`，`learning by watching(观察学习)`。在 Imitation learning 里面，你有一些专家 expert 的示范，agent 也可以跟环境互动，但它没有办法从环境里面得到任何的 reward，它只能看着专家的示范来学习什么是好，什么是不好。

其实，多数的情况，我们都没有办法真的从环境里面得到非常明确的 reward。举例来说，如果是棋类游戏或者是电玩，你有非常明确的 reward。但是其实多数的任务，都是没有 reward 的。以 chat-bot 为例，**机器跟人聊天，聊得怎么样算是好，聊得怎么样算是不好，你无法给出明确的 reward**。

虽然没有办法给出 reward，但是收集专家 expert 的示范 demonstration 是可以做到的。举例来说，

- 在自动驾驶汽车里面，虽然你没有办法给出自动驾驶汽车的 reward，但你可以收集很多人类开车的纪录。
- 在 chat-bot 里面，你可能没有办法定义什么叫做好的对话，什么叫做不好的对话。但是收集很多人的对话当作范例，这一件事情也是可行的。

所以 imitation learning 的使用性非常高。在 imitation learning 里面，我们介绍两个方法。第一个叫做 `行为克隆 Behavior Cloning`，第二个叫做 `逆向强化学习 Inverse Reinforcement Learning` 或者又叫做 `Inverse Optimal Control`。

## 2. 行为克隆 Behavior Cloning

### ① 概念

**其实 `Behavior Cloning` 跟监督学习是一模一样的**。以自动驾驶汽车为例，你可以收集到人开自动驾驶汽车的所有资料，比如说可以通过行车记录器进行收集。看到这样的场景的时候，人会决定向前。机器就采取跟人一样的行为，也向前，就结束了。**这个就叫做 Behavior Cloning，Expert 做什么，机器就做一模一样的事。**

<img src="https://gitee.com/veal98/images/raw/master/img/20201029114613.png" style="zoom: 50%;" />

怎么让机器学会跟 expert 一模一样的行为呢？就把它当作一个 supervised learning 的问题，你去收集很多行车记录器，然后再收集人在那个情境下会采取什么样的行为。你知道说人在 state $s_1$ 会采取 action $a_1$，人在 state $s_2$ 会采取 action $a_2$。人在 state $s_3$ 会采取 action $a_3$。接下来，你就训练一个 network。这个 network 就是你的 actor，它输入 $s_i$ 的时候，你就希望它的输出是 $a_i$

### ② DataSet Aggregation

Behavior Cloning 虽然非常简单，但它的问题是如果你只收集 expert 的资料，看过的场景会是非常有限的。

所以光是做 Behavior Cloning 是不够的，**只观察 expert 的行为是不够的，我们还需要 `Dataset Aggregation`**。

以自动驾驶汽车为例的话，假设一开始，你的 actor 叫作 π1，你让 π1 去开这个车。但车上坐了一个 expert。这个 expert 会不断地告诉 π1，如果在这个情境里面，我会怎么样开，但是 π1 自己开自己的。比如说，如上图所示，一开始的时候，expert 可能说往前走。在拐弯的时候，expert 可能就会说往右转。但 π1 是不管 expert 的指令的，所以它会继续去撞墙。我们要做的记录就是 expert 在 π1 看到的这种场景的情况下会做出什么样的行为，然后用这个数据去训练新的 actor $\pi_2$ 。这个过程反复继续下去就叫做 `Dataset Aggregation`。

<img src="https://gitee.com/veal98/images/raw/master/img/20201029122503.png" style="zoom:50%;" />

🚨 agent 可能会复制专家**所有的动作**，包括一些无关的动作。但是机器只有有限的学习能力，可能会导致它复制错误的行为。有些行为必须被复制，但有些可以被忽略，但监督学习对所有的误差都平等处理。

在监督学习中，我们希望训练数据和测试数据有相同的分布，但是在行为克隆中，训练数据来自于专家的分布，而测试数据来自于 actor，因为专家的 π 和 actor 的是不一样的，生成的 state 也是不一样的，分布可能会不相同，因此引入 IRL 👇 

## 3. 逆向强化学习 IRL

在前面介绍过的 RL 中：环境和 reward 是用来生成一个 actor 的，但是在 IRL 中，没有 reward function，也就是说**机器是可以跟环境互动的，但它得不到 reward**。它的 reward 必须要从 expert 那边推出来，有了环境和 expert 示范 demonstration 以后，去反推出 reward function。然后这个 reward function 才会被用来训练 actor。

<img src="https://gitee.com/veal98/images/raw/master/img/20201029142407.png" style="zoom:50%;" />

Inverse Reinforcement Learning 实际上是怎么做的呢？

Expert 去玩一玩游戏，得到这一些游戏的纪录，你的 actor 也去玩一玩游戏，得到这些游戏的纪录。接下来，你要定一个 reward function，**这个 reward function 的原则就是 expert 得到的分数要比 actor 得到的分数高**。有了新的 reward function 以后，就可以套用一般 Reinforcement Learning 的方法去学习一个 actor，这个 actor 会针对 reward function 去最大化它的 reward，采取一大堆的动作。

接下来，我们接着改变 reward function。这个 actor 就会很生气，它已经可以在这个 reward function 得到高分。**但是它得到高分以后，我们就改 reward function，仍然让 expert 可以得到比 actor 更高的分数**。这个就是 `Inverse Reinforcement learning`。有了新的 reward function 以后，根据这个新的 reward function，你就可以得到新的 actor，新的 actor 再去跟环境做一下互动，它跟环境做互动以后， 你又会重新定义你的 reward function，让 expert 得到的 reward 比 actor 大。

也就是说：**专家永远是最棒的** 😊

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
# 💠 强化学习概述

---

## 1. 什么是强化学习 Reinforcement Learning

<img src="https://gitee.com/veal98/images/raw/master/img/20201020170513.png" style="zoom: 45%;" />

⭐ **强化学习讨论的问题是一个 智能体 (`agent` / `actor`) 怎么在一个 复杂不确定的环境 `environment` 里面 去 极大化 它能获得的 奖励 (`reward`)。**

> 💡 李宏毅老师的课程中将智能体称为 `actor`

示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到 **state (状态)**，agent 会利用这个状态输出一个 **action (动作)**。然后这个动作会放到环境之中去，环境会通过这个 agent 采取的动作，输出下一个状态以及当前的这个动作得到的**奖励 (reward)**。<u>Agent 的目的就是为了尽可能多地从环境中获取奖励</u>。

## 2. 强化学习和监督学习

先看一下强化学习和机器学习的关系：

<img src="https://gitee.com/veal98/images/raw/master/img/20201021091905.png" style="zoom: 67%;" />

我们可以**把强化学习跟监督学习做一个对比**。举个图片分类的例子，监督学习就是说我们有一大堆标定的数据，比如车、飞机、凳子这些标定的图片，**这些图片都要满足独立同分布，就是它们之间是没有关联的一个分布**。然后我们训练一个分类器，比如说右边这个神经网络。为了分辨出这个图片是车辆还是飞机，训练过程中，我们把真实的 label 给了这个网络。当这个网络做出一个错误的预测，比如现在输入了这个汽车的图片，它预测出来是飞机。我们就会直接告诉它，你这个预测是错误的，正确的 label 应该是车。然后我们把这个错误写成一个`损失函数(loss function)`，通过 Backpropagation 来训练这个网络。

<img src="https://gitee.com/veal98/images/raw/master/img/20201020171713.png" style="zoom: 45%;" />

所以在监督学习过程中，有两个假设，

- 输入的数据，标定的数据，它都是没有关联的，尽可能没有关联。因为如果有关联的话，这个网络是不好学习的。
- 我们告诉这个 learner 正确的标签是什么，这样它可以通过正确的标签来修正自己的这个预测。

**在强化学习里面，这两点其实都不满足**。举一个 Atari Breakout 游戏的例子，这是一个打砖块的游戏，控制木板，然后把这个球反弹到上面来消除这些砖块：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201007.png" style="zoom: 40%;" />

- 在游戏过程中，大家可以发现这个 **agent 得到的观测不是独立同分布，上一帧下一帧其实有非常强的连续性**。
- 另外一点，在玩游戏的过程中，你**并没有立刻获得这个反馈**。比如你现在把这个木板往右移，那么只会使得这个球往上或者往左上去一点，你并不会得到立刻的反馈。所以强化学习这么困难的原因是没有得到很好的反馈，但是你依然希望这个 agent 在这个环境里面学习。

强化学习的训练数据就是这样一个玩游戏的过程。你从第一步开始，采取一个决策，比如说你把这个往右移，接到这个球了。第二步你又做出决策，得到的 training data 是一个玩游戏的序列。比如现在是在第三步，你把这个序列放进去，你希望这个网络可以输出一个决策，在当前的这个状态应该输出往右移或者往左移：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020201812.png" style="zoom:50%;" />

这里有个问题，就是**我们没有标签来说明你现在这个动作是正确还是错误，必须等到这个游戏结束才能知道现在这个动作到底是不是对最后的输赢有帮助**。这里就面临一个 `延迟奖励(Delayed Reward)`，所以就使得训练这个网络非常困难。

🚩 **总结下强化学习和监督学习的区别**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020202907.png" style="zoom:40%;" />

- 🔸 首先强化学习输入的序列的数据并不是像 supervised learning 里面这些样本都是独立同分布的。

- 🔸 另外一点是 learner 并没有被告诉你每一步正确的行为应该是什么。Learner 不得不自己去发现哪些行为可以使得它最后得到这个奖励，只能通过不停地尝试来发现最有利的 action。

- 🔸 还有一点是 agent 获得自己能力的过程中，其实是通过不断地 **试错(trial-and-error exploration)**。Exploration 和 exploitation 是强化学习里面非常核心的一个问题。

  - **`Exploration 探索 ` 是说你会去尝试一些【新的行为】，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。**

  - **`Exploitation 利用 ` 说的是你就是就采取你【已知】的可以获得最大奖励的行为，你就重复执行这个 action 就可以了，因为你已经知道可以获得一定的奖励。**

    > 💡 比如说我们选择吃饭的餐馆：Environment 可理解为你可以选择的餐馆，agent 代表你，Reward 就是吃到好吃的美食：
    >
    > - `Exploitation `是去你喜欢的店吃，可以吃到味道不错的；
    >
    > - 而 `Exploration `代表尝试一家新的餐馆，这就意味着可能 reward 并不一定高

  因此，我们需要在 exploration 和 exploitation 之间取得一个**权衡（trade-off）**，这也是在监督学习里面没有的情况。

- 🔸 在强化学习过程中，没有非常强的 supervisor，只有一个 `奖励信号(reward signal)`，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent 在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。

  当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就比如说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后才告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。

### Exploration and Exploitation 

在强化学习里面，`Exploration` 和` Exploitation` 是两个很核心的问题。上面已经说过了，此处只是为了目录的完整性列出来 😊

#### K-armed Bandit K-臂赌博机

与一般监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：**最大化单步奖赏**，即仅考虑一步操作。需注意的是，即便在这样的简化情形下，强化学习仍与监督学习有显著不同，**因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作**。

想要最大化单步奖赏需考虑两个方面：一是需知道每个动作带来的奖赏，二是要执行奖赏最大的动作。若每个动作对应的奖赏是一个确定值，那么尝试遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值。

🚩 实际上，单步强化学习任务对应了一个理论模型，即 ` K-臂赌博机(K-armed bandit)`。K-臂赌博机也被称为 `多臂赌博机(Multi-armed bandit) `。如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020225718.png" style="zoom: 67%;" />

K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。

- 若仅为获知每个摇臂的期望奖赏，则可采用 `仅探索(exploration-only)法` ：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。
- 若仅为执行奖赏最大的动作，则可采用 `仅利用(exploitation-only)法`：按下目前最优的(即到目前为止平均奖赏最大的)摇臂，若有多个摇臂同为最优，则从中随机选取一个。

显然，仅探索法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。

⭐ **事实上，探索 (即估计摇臂的优劣) 和 利用 (即选择当前最优摇臂) 这两者是矛盾的，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的`探索-利用窘境(Exploration-Exploitation dilemma)`。显然，想要累积奖赏最大，则必须在探索与利用之间达成较好的折中**。

## 3. 强化学习的特征

通过跟监督学习比较，我们可以总结出强化学习的一些特征：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020203134.png" style="zoom:40%;" />

- 首先它是有 `trial-and-error exploration`，它需要通过探索环境来获取对这个环境的理解。
- 第二点是强化学习 agent 会从环境里面获得 **延迟奖励** `Delayed reward`。
- 第三点是**在强化学习的训练过程中，时间非常重要**。因为你得到的数据都是有这个时间关联的，而不是独立同分布的。<u>在机器学习中，如果观测数据有非常强的关联，其实会使得这个训练非常不稳定。这也是为什么在监督学习中，我们希望 data 尽量是独立同分布了，这样就可以消除数据之间的相关性</u>。
- 第四点是 **agent 的行为会影响它随后得到的数据**，这一点是非常重要的。在我们训练 agent 的过程中，很多时候我们也是通过正在学习的这个 agent 去跟环境交互来得到数据。所以如果在训练过程中，这个 agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的问题就是<u>怎么让这个 agent 的行为一直稳定地提升</u>。

## 4. Why Reinforcement Learning

为什么我们关注这个强化学习，其中非常重要的一点就是**强化学习得到的这个模型可以取得超人类的结果**。监督学习获取的这些监督数据，其实是让人来标定的。比如说 ImageNet，这些图片都是人类标定的。那么我们就可以确定这个算法的 upper bound (上限)，**人类的这个标定结果决定了它永远不可能超越人类**。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人的能力的这个表现，比如说 AlphaGo。

💬 这里给大家举一些在现实生活中强化学习的例子：

- 国际象棋是一个强化学习的过程，因为这个棋手就是在做出一个选择来跟对方对战。
- 在自然界中，羚羊其实也是在做一个强化学习，它刚刚出生的时候，可能都不知道怎么站立，然后它通过 `trial- and-error` 的一个尝试，三十分钟过后，它就可以跑到每小时 36 公里，很快地适应了这个环境。
- 你也可以把股票交易看成一个强化学习的问题，怎么去买卖来使你的收益极大化。

## 5. 深度强化学习 Deep Reinforcement Learning

强化学习是有一定的历史的，只是大家**把强化学习跟深度学习结合起来**，就形成了 **深度强化学习(Deep Reinforcemet Learning)**：

<img src="https://gitee.com/veal98/images/raw/master/img/20201020212120.png" style="zoom:40%;" />

- 之前的强化学习，它其实是先设计特征，然后训练代价函数的一个过程，就是它先设计了很多手工的特征，这个手工特征可以描述现在整个状态。得到这些特征过后，它就可以通过训练一个分类网络或者分别训练一个代价函数来做出决策。
- 现在我们有了深度学习，有了神经网络，那么大家也把这个过程改进成一个 **端到端的训练 end-to-end training** 的过程。**你直接输入这个状态，我们不需要去手工地设计这个特征，就可以让它直接输出 action**。那么就可以用一个神经网络来拟合我们这里的 value function 或 policy network，省去 了 特征工程 feature engineering 的过程。

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
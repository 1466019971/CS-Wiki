# 🌐 稀疏奖励 Sparse Reward

---

当 reward 的分布非常分散时，对于机器而言学习如何行动会十分困难。

比如说要让一个机器人倒水进水杯里，如果不对机器人做任何指导，可能它做很多次尝试，reward都一直是零。（不知道杯子在哪，不知道拿着手上的水壶干嘛，不知道水壶靠近杯子之后应该怎么做）

因此，在训练或指导一个 actor 去做你想要它做的事情时，我们可以从三个方面去解决 Sparse Reward 问题：

- Reward Shaping
- Curriculum Learning
- Hierarchical RL

## 1. Reward Shaping

🚩 **Reward shaping 的意思是说环境有一个固定的 reward，它是真正的 reward，但是为了让 agent 学出来的结果是我们要的样子，我们刻意地设计了一些 reward 来引导我们的 agent。**

💬 举例来说，如果是把小孩当成一个 agent 的话。他可以做两个 actions，一个 action 是他可以出去玩，在下一秒钟它会得到 reward 1。但是他在月考的时候，成绩可能会很差。所以在100 个小时之后呢，他会得到 reward -100。另一个 action 是他也可以决定要念书，然后在下一个时间，因为他没有出去玩，他觉得很不爽，所以他得到 reward -1。但是在 100 个小时后，他可以得到 reward 100。

但对一个小孩来说，他可能就会想要 play 而不是 study。我们计算的是 accumulated reward，但对小孩来说，他并不在意未来的 reward。所以这时候大人就要引导他，怎么引导呢？就骗他说，如果你坐下来念书我就给你吃一个棒棒糖。所以，对他来说，下一个时间点会得到的 reward 就变成是 positive 的。所以他就觉得 study 是比 play 好的。虽然这并不是真正的 reward，而是其他人骗他的 reward，告诉他说你采取这个 action 是好的。

其中一个技术是给 machine 加上 `curiosity`，所以叫 `curiosity driven reward`。如下图所示，在原来的模型当中，actor 与环境做互动，根据环境给的 state，采取一定的 action，并得到 reward。而**新的模型引入了一个新的函数：ICM**（图中的橙色部分）

<img src="https://gitee.com/veal98/images/raw/master/img/20201029153005.png" style="zoom: 50%;" />

在 curiosity driven 的这种技术里面，你会加上一个新的 reward function。这个新的 reward function 叫做 `ICM(intrinsic curiosity module)`，它就是要给机器加上好奇心。

**ICM 函数**将 $s_t, a_t, s_{t+1}$ 作为输入，得到一种新的 reward $r^i$，**actor 的目标现在变为最大化两个部分的 reward**，即 r 和 $r^i$。可以理解为，**新的模型中，actor 被鼓励基于好奇去采取不一样的行动，这样的好奇带来的行动可以给 actor 带来额外的收益**。

<img src="https://gitee.com/veal98/images/raw/master/img/20201029153747.png" style="zoom:40%;" />

在这个模型中，两个网络 network1 和 network2 是单独进行训练的。

黄色的格子代表 `feature extractor`，它是输入一个状态，然后输出一个 feature vector 来代表这个状态，这个 feature extractor 可以把那种没有意义的画面，比如说风吹草动、白云的飘动、树叶的飘动这种没有意义的东西直接把它过滤掉

## 2. 课程式学习 Curriculum Learning

### ① 概念

`curriculum learning` 不是 reinforcement learning 所独有的概念，其实在 machine learning，尤其是 deep learning 里面，你都会用到 curriculum learning 的概念。举例来说，你为机器的学习做规划，**你给他喂训练数据的时候，是有顺序的，通常都是由简单到难**。就好比说，假设你今天要交一个小朋友作微积分，他做错就打他一巴掌，这样他永远都不会做对，太难了。你要先教他九九乘法，然后才教他微积分。所以 curriculum learning 的意思就是在教机器的时候，从简单的题目教到难的题目。

### ② 反向课程生成 Reverse Curriculum Generation

反向课程生成是一个帮机器设计课程比较通用的方法

<img src="https://gitee.com/veal98/images/raw/master/img/20201029204525.png" style="zoom:40%;" />

给定一个目标 state $s_g$，在 $s_g$ 的附近取一些靠近 $s_g$ 的样本 $s_1$，从 $s_1$ 开始，每个 trajectory 都会有一个 reward $R(s_1)$，删除那些 reward 太大（意味着已经学会了，课程太简单）的 $s_1$，以及那些太小的 $s_1$（意味着当前对于机器来说这个课程难度太大），然后继续以 $s_1$ 为中心采样，重复上述过程。

## 3. Hierarchical RL 级联强化学习

级联强化学习也就是分层的 reinforcement learning。我们有好几个 agent然后，有一些 agent 负责比较 high level 的东西，它负责订目标，然后它订完目标以后，再分配给其他的负责 low-level 的 agent 把它执行完成。

- 如果低一层的 agent 没法达到目标，那么高一层的 agent 会受到惩罚（高层 agent 将自己的愿景传达给底层agent）

- 如果一个 agent 到了一个错误的目标，那就假设最初的目标本来就是一个错误的目标（保证已经实现的成果不被浪费）

<img src="https://gitee.com/veal98/images/raw/master/img/20201029205324.png" style="zoom:50%;" />

## 📚 References

- [Bilibili - 李宏毅《深度强化学习》](https://www.bilibili.com/video/BV1MW411w79n)
- [Github - LeeDeepRL - Notes](https://datawhalechina.github.io/leedeeprl-notes/)
- [CSDN - 李宏毅深度强化学习笔记 - jessie](https://blog.csdn.net/cindy_1102/article/details/87904928)
- [强化学习纲要](https://github.com/zhoubolei/introRL)
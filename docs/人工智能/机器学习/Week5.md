# 🎳 神经网络：进阶学习 Neural Networks: Learning

---

## 1. 代价函数 Cost Function 

首先引入一些便于稍后讨论的新标记方法：

假设神经网络的训练样本有 m 个，每个包含一组输入 x 和一组输出信号 y ，**L 表示神经网络层数，$S_l$ 表示每层的神经元个数（比如 $S_1$ 表示第一层神经元的个数），$S_L$代表最后一层（输出层）中处理单元的个数。**

将神经网络的分类定义为两种情况：二类分类和多类分类：

- 二类分类：$S_L = 1$ 

- K 类分类：$S_L = K (K ≥ 3)$ 

<img src="https://gitee.com/veal98/images/raw/master/img/20200605204514.png" style="zoom:50%;" />

我们回顾逻辑回归问题中我们的代价函数为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605205040.png"  />

在逻辑回归中，我们只有一个输出变量，又称标量（**scalar**），也只有一个因变量 y ，但是在神经网络中，我们可以有很多输出变量，我们的 $h_θ(x)$ 是一个维度为 K 的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605205228.png"  />

这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。

## 2. 反向传播算法 Backpropagation Algorithm

接下来我们学习一种**让代价函数最小化的算法：反向传播算法**

之前我们在计算神经网络预测结果的时候我们采用了一种前向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的 $h_θ(x)$。

现在，为了计算代价函数的偏导数 <img src="https://gitee.com/veal98/images/raw/master/img/20200605210016.png"  />，我们需要采用一种反向传播算法，也就是 🔴 **首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层**。 以一个例子来说明反向传播算法：

**假设我们的训练集只有一个样本 （x , y）**，我们的神经网络是一个四层的神经网络，其中 K = 4，$S_L = 4$，L = 4：

- **前向传播算法**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20200605210157.png" style="zoom:50%;" />



- **反向传播算法**：

  我们从最后一层的误差开始计算，误差是激活单元的预测（$a^{(4)}$）与实际值（$y^k$）之间的误差x。 我们用 `δ` 来表示误差，则： <img src="https://gitee.com/veal98/images/raw/master/img/20200605212154.png"  /> 

  我们利用这个误差值来计算前一层的误差：<img src="https://gitee.com/veal98/images/raw/master/img/20200605212213.png"  />  其中<img src="https://gitee.com/veal98/images/raw/master/img/20200605212308.png"  /> 是 Sigmod 函数的导数，<img src="https://gitee.com/veal98/images/raw/master/img/20200605212329.png"  />。而 <img src="https://gitee.com/veal98/images/raw/master/img/20200605212346.png"  /> 则是权重导致的误差的和。

  下一步是继续计算第二层的误差： <img src="C:\Users\19124\AppData\Roaming\Typora\typora-user-images\image-20200605212407324.png" alt="image-20200605212407324"  />

  因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设，即我们不做任何正则化处理时有：

   ![](https://gitee.com/veal98/images/raw/master/img/20200605212428.png)

  重要的是清楚地知道上面式子中上下标的含义：

  - l 代表目前所计算的是第几层。

  - j 代表目前计算层中的激活单元的下标，也将是下一层的第 j 个输入变量的下标。

  - i 代表下一层中误差单元的下标，是受到权重矩阵中第 i 行影响的下一层中的误差单元的下标。



在上面的特殊情况中（只有一个训练集），我们需要计算每一层的误差单元来计算代价函数的偏导数。现在 🚩 **我们将其推广到具有 m 个训练集的情况**：我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用 $△_{ij}^{(l)}$ 来表示这个误差矩阵。第 l 层的第 i 个激活单元受到第 j 个参数影响而导致的误差。

我们的算法表示为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605213248.png" style="zoom:50%;" />

⭐ 即**首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。**

在求出了 $△_{ij}^{(l)}$ 之后，我们便可以计算 ⭐ **代价函数的偏导数**了，计算方法如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605213406.png"  />



## 3. BP算法的直观理解 Backpropagation Intuition

为了更好地理解反向传播算法，我们再来仔细研究一下前向传播的原理：

前向传播算法：

<img src="https://gitee.com/veal98/images/raw/master/img/20200606144914.png" style="zoom:50%;" />

反向传播算法：

<img src="https://gitee.com/veal98/images/raw/master/img/20200606145652.png" style="zoom:50%;" />



## 4. BP算法的实现细节：展开参数 Implementation Note_ Unrolling Parameters

在上一段视频中，我们谈到了怎样使用反向传播算法计算代价函数的导数。在这段视频中，我想快速地向你介绍一个细节的实现过程，**怎样把你的参数从矩阵展开成向量**，以便我们在高级最优化步骤中的使用需要。

![](https://gitee.com/veal98/images/raw/master/img/20200606150641.png)

![](https://gitee.com/veal98/images/raw/master/img/20200606150656.png)

![](https://gitee.com/veal98/images/raw/master/img/20200606150707.png)

## 5. 梯度检验 Gradient Checking

当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。（<u>大部分原因都是 BP 算法自身的 Bug</u>）

为了避免这样的问题，我们采取一种叫做梯度的数值检验（**Numerical Gradient Checking**）方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。

对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 θ，我们计算出在 θ-ε 处和 θ+ε  的代价值（ε 是一个非常小的值，通常选取 0.0001），然后求两个代价的平均，用以估计在 θ 处的代价值。

<img src="https://gitee.com/veal98/images/raw/master/img/20200606151736.png" style="zoom:50%;" />

当是一个向量时，我们则需要对偏导数进行检验。因为代价函数的偏导数检验只针对一个参数的改变进行检验：

<img src="https://gitee.com/veal98/images/raw/master/img/20200606152115.png" style="zoom:50%;" />



🚨  **在运行你的代码进行学习或者说训练网络之前，务必关掉梯度检验**。因为梯度检验的代码是一个计算量非常大的，也是非常慢的计算导数的程序。而 BP 算法是一个高性能的计算导数的方法，一旦你通过梯度校验证明反向传播的实现是正确的，就应该关掉梯度校验。

## 6. 随机初始化 Random Initialization

任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。**如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为同一个非 0 的数，结果也是一样的。**

🚩 我们通常初始参数为正负 ε （这里的 ε 和梯度校验的 ε 没有任何关系 ）之间的随机值，假设我们要随机初始一个尺寸为 10×11 的参数矩阵，代码如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200606153226.png" style="zoom: 50%;" />

## 7. 小结 Putting It Together

总结一下使用神经网络时的步骤：

- **第一件要做的事是选择网络结构**，即决定选择多少层以及决定每层分别有多少个单元。

  <img src="https://gitee.com/veal98/images/raw/master/img/20200606153649.png" style="zoom:50%;" />

  第一层的单元数即我们训练集的特征数量。

  最后一层的单元数是我们训练集的结果的类的数量。

  如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。

- 接下来就是**训练神经网络**：

  - 参数的随机初始化

  - 利用正向传播方法计算所有的 $h_θ(x)$

  - 编写计算代价函数 J 的代码

  - 利用反向传播方法计算所有偏导数

  - 利用梯度检验方法检验这些偏导数

  - 使用优化算法（比如梯度下降算法或者更高级的优化方法）来最小化代价函数
  
    🚨 神经网络中的代价函数是非凸函数，也就是说梯度下降算法或者其他更高级的优化方法得到的都有可能只是局部最小值，<u>但这不是个大问题</u>，因为这些算法在一般情况下都能得到一个比较小的局部最小值，尽管它可能不是全局最小值

## ✍ Quiz

### ① 第 1 题

您正在训练一个三层神经网络，希望使用反向传播来计算代价函数的梯度。 在反向传播算法中，其中一个步骤是更新 $\Delta^{(2)}_{ij} := \Delta^{(2)}_{ij} +  \delta^{(3)}_i * (a^{(2)})_j$ 对于每个i，j，下面哪一个是这个步骤的正确矢量化？

- $\Delta^{(2)} := \Delta^{(2)} +  (a^{(2)})^T * \delta^{(3)}$

- $\Delta^{(2)} := \Delta^{(2)} + (a^{(3)})^T * \delta^{(2)}$

- ✅ $\Delta^{(2)} := \Delta^{(2)} +  \delta^{(3)} * (a^{(2)})^T$

- $\Delta^{(2)} := \Delta^{(2)} +  \delta^{(3)} * (a^{(3)})^T$

### ② 第 2 题

假设`Theta1`是一个 5x3 矩阵，`Theta2`是一个 4x6 矩阵。令`thetaVec=[Theta1(;);Theta2(:)]`。下列哪一项可以正确地还原`Theta2`？

- ✅ `reshape(thetaVec(16:39),4,6)` 

  5x3 + 4x6 = 39

- `reshape(thetaVec(15:38),4,6)` 

- `reshape(thetaVec(16:24),4,6)` 

-  `reshape(thetaVec(15:39),4,6)` 

-   `reshape(thetaVec(16:39),6,4)`

### ③ 第 3 题

<img src="https://gitee.com/veal98/images/raw/master/img/20200606161058.png"  />

- 8 
-  6 
-  5.9998 
- ✅ 6.0002

### ④ 第 4 题

以下哪项陈述是正确的？选择所有正确项

- 使用较大的 λ 值不会影响神经网络的性能；我们不将 λ 设置为太大的唯一原因是避免数值问题

- 如果我们使用梯度下降作为优化算法，梯度检查是有用的。然而，如果我们使用一种先进的优化方法（例如在fminunc中），它没有多大用处

- ✅ 使用梯度检查可以帮助验证反向传播的实现是否没有 bug

- ✅ 如果我们的神经网络过拟合训练集，一个合理的步骤是增加正则化参数 λ

### ⑤ 第 5 题

以下哪项陈述是正确的？选择所有正确项

- 假设参数$Θ^{(1)}$是一个方矩阵（即行数等于列数）。如果我们用它的转置代${Θ^{(1)}}^T$替$Θ^{(1)}$，那么我们并没有改变网络正在计算的功能。

- ✅ 假设我们有一个正确的反向传播实现，并且正在使用梯度下降训练一个神经网络。假设我们将 J(Θ) 绘制为迭代次数的函数，并且发现它是递增的而不是递减的。一个可能的原因是学习率 α 太大。

- 假设我们使用学习率为 α 的梯度下降。对于逻辑回归和线性回归，J(Θ) 是一个凸优化问题，因此我们不想选择过大的学习率 α。 然而，对于神经网络，J(Θ)可能不是凸的，因此选择一个非常大的 α 值只能加快收敛速度。

- ✅ 如果我们使用梯度下降训练一个神经网络，一个合理的调试步骤是将 J(Θ) 绘制为迭代次数的函数，并确保每次迭代后它是递减的（或至少是不递增的）。

---

# 📚 References

- 🤖 [吴恩达机器学习经典名课【中英字幕】](https://www.bilibili.com/video/BV164411S78V?p=2)

- 💠 [黄海广 - 斯坦福大学2014机器学习教程中文笔记](http://www.ai-start.com/ml2014/)

- 🍧 [90题细品吴恩达《机器学习》，感受被刷题支配的恐惧](https://www.kesci.com/home/project/5e0f01282823a10036b280a7)

- 🥩 [吴恩达机器学习 课后实验 python实现](https://www.kesci.com/home/project/5da16a37037db3002d441810)
# 🎳 神经网络：进阶学习 Neural Networks: Learning

---

## 1. 代价函数 Cost Function 

首先引入一些便于稍后讨论的新标记方法：

假设神经网络的训练样本有 m 个，每个包含一组输入 x 和一组输出信号 y ，**L 表示神经网络层数，$S_l$ 表示每层的神经元个数（比如 $S_1$ 表示第一层神经元的个数），$S_L$代表最后一层（输出层）中处理单元的个数。**

将神经网络的分类定义为两种情况：二类分类和多类分类：

- 二类分类：$S_L = 1$ 

- K 类分类：$S_L = K (K ≥ 3)$ 

<img src="https://gitee.com/veal98/images/raw/master/img/20200605204514.png" style="zoom:50%;" />

我们回顾逻辑回归问题中我们的代价函数为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605205040.png"  />

在逻辑回归中，我们只有一个输出变量，又称标量（**scalar**），也只有一个因变量 y ，但是在神经网络中，我们可以有很多输出变量，我们的 $h_θ(x)$ 是一个维度为 K 的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605205228.png"  />

这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出 K 个预测，基本上我们可以利用循环，对每一行特征都预测 K 个不同结果，然后在利用循环在 K 个预测中选择可能性最高的一个，将其与 y 中的实际数据进行比较。

## 2. 反向传播算法 Backpropagation Algorithm

接下来我们学习一种**让代价函数最小化的算法：反向传播算法**

之前我们在计算神经网络预测结果的时候我们采用了一种前向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的 $h_θ(x)$。

现在，为了计算代价函数的偏导数 <img src="https://gitee.com/veal98/images/raw/master/img/20200605210016.png"  />，我们需要采用一种反向传播算法，也就是 🔴 **首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层**。 以一个例子来说明反向传播算法：

**假设我们的训练集只有一个样本 （x , y）**，我们的神经网络是一个四层的神经网络，其中 K = 4，$S_L = 4$，L = 4：

- **前向传播算法**：

  <img src="https://gitee.com/veal98/images/raw/master/img/20200605210157.png" style="zoom:50%;" />



- **反向传播算法**：

  我们从最后一层的误差开始计算，误差是激活单元的预测（$a^{(4)}$）与实际值（$y^k$）之间的误差x。 我们用 `δ` 来表示误差，则： <img src="https://gitee.com/veal98/images/raw/master/img/20200605212154.png"  /> 

  我们利用这个误差值来计算前一层的误差：<img src="https://gitee.com/veal98/images/raw/master/img/20200605212213.png"  />  其中<img src="https://gitee.com/veal98/images/raw/master/img/20200605212308.png"  /> 是 Sigmod 函数的导数，<img src="https://gitee.com/veal98/images/raw/master/img/20200605212329.png"  />。而 <img src="https://gitee.com/veal98/images/raw/master/img/20200605212346.png"  /> 则是权重导致的误差的和。

  下一步是继续计算第二层的误差： <img src="C:\Users\19124\AppData\Roaming\Typora\typora-user-images\image-20200605212407324.png" alt="image-20200605212407324"  />

  因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设，即我们不做任何正则化处理时有：

   ![](https://gitee.com/veal98/images/raw/master/img/20200605212428.png)

  重要的是清楚地知道上面式子中上下标的含义：

  - l 代表目前所计算的是第几层。

  - j 代表目前计算层中的激活单元的下标，也将是下一层的第 j 个输入变量的下标。

  - i 代表下一层中误差单元的下标，是受到权重矩阵中第 i 行影响的下一层中的误差单元的下标。



在上面的特殊情况中（只有一个训练集），我们需要计算每一层的误差单元来计算代价函数的偏导数。现在 🚩 **我们将其推广到具有 m 个训练集的情况**：我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用 $△_{ij}^{(l)}$ 来表示这个误差矩阵。第 l 层的第 i 个激活单元受到第 j 个参数影响而导致的误差。

我们的算法表示为：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605213248.png" style="zoom:50%;" />

⭐ 即**首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。**

在求出了 $△_{ij}^{(l)}$ 之后，我们便可以计算 ⭐ **代价函数的偏导数**了，计算方法如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200605213406.png"  />



## 3. 反向传播算法的直观理解

## 4. 实现注意：展开参数

## 5. 梯度检验

## 6. 随机初始化

## 7. 综合起来

## 8. 自主驾驶



---

# 📚 References

- 🤖 [吴恩达机器学习经典名课【中英字幕】](https://www.bilibili.com/video/BV164411S78V?p=2)

- 💠 [黄海广 - 斯坦福大学2014机器学习教程中文笔记](http://www.ai-start.com/ml2014/)

- 🍧 [90题细品吴恩达《机器学习》，感受被刷题支配的恐惧](https://www.kesci.com/home/project/5e0f01282823a10036b280a7)
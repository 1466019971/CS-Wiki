# 一、引言

## 1. 什么是机器学习 Machine Learning

### ① 定义

🔵 **Arthur Samuel (1959)**：Machine Learning : Field of study that gives computers the ability to learn without being explicitly programmed. 在没有明确设置的情况下，使计算机具有学习能力的研究领域。

🔴 **Tom Mitchell (1998)**：Well-posed Learning Problem：A computer program is said to learn from experience E with respect to some task T and some performance measure P，if its performance on T，as measured by P，improves with experience E. 一个适当的学习问题定义如下：计算机程序从经验 E （程序与自己下上万次跳棋）中学习，解决某一任务 T （玩跳棋），进行某一性能度量 P （与新对手玩跳棋时赢的概率），通过 P 测定在 T 上的表现因经验 E 而提高。

### ② 每节小问

❓ <u>基于 Tom Mitchell 对于机器学习的定义，提出一个问题：</u>

Suppose your email program watches which emails you do or do not mark as spam（垃圾邮件），and based on that learns how to better filter spam. What is the task T in this setting? 假设你的邮箱正在观察你将哪些邮件标记为垃圾邮件，并基于此学习如何更好的过滤邮件，那么在这里任务 T 是什么？

- ✅ Classifying emails as spam or not spam 把邮件分类为垃圾邮件和非垃圾邮件：这是任务 T

- ❌ Watching you label email as spam or not spam 是否把邮件标记为垃圾邮件：这是经验 E

- ❌ The number(or fraction) of eamils correctly classified as spam/not spam：正确归类邮件的数量/比例：这是性能度量 P

即**任务 T 在得到经验 E 之后会提高性能度量 P**

### ③ 机器学习算法概览

📜 **Machine Learning algorithms**：

- **Supervised  learning** 监督学习：我们教计算机学习

- **Unsupervised learning** 无监督学习：计算机自己学习

Others：`Reinforcement learning` 强化学习，`recommender systems` 推荐系统

> 🔖 Also talk about : Practical advice for applying for applying learning algorithms 本课程中还会讲到应用学习算法时的实际建议 

---



## 2. 监督学习 Supervised Learning

### ① 定义

🔴 **Supervised learning**：We give the algorithm a data set，in which the "right answers" were given，and the task of the algorithm was to just produce more of these right answers. **我们给算法一个数据集，其中包含了正确答案，算法的目的就是给出更多的正确答案。**

### ② 回归问题 Regression

💬 举例来说，比如有个人想要卖房子，他需要结合自己房子的大小来评估房子能卖多少钱。此时我们已经有了一些正确的数据集（房子大小和房价的对应关系），如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200525222706.png" style="zoom: 50%;" />

那么学习算法能做到的一件事情就是用一条直线或者二次函数来拟合数据，从而预测出房子能卖多少钱。后面我们要讨论的问题就是选择使用直线拟合数据还是选用二次函数来拟合数据。

<img src="https://gitee.com/veal98/images/raw/master/img/20200525222851.png" style="zoom:50%;" />



🔴 用更专业的术语来定义，它也被称为**回归问题 Regression**：**回归的目标就是** Predict continuous valued output **预测连续的数值输出**

### ③ 分类问题 Classification

💬 我们来看下一个例子：比如我们要预测乳腺癌是恶性的还是良性的，假设此时我们已有一些正确的数据集（肿瘤的大小 tumor size 和肿瘤良/恶性 malignant 的对应关系），如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200525223553.png" style="zoom:50%;" />

那么机器学习的目标就是根据肿瘤的大小估计出肿瘤是良性还是恶性的概率。用更专业的术语来讲，这就是**一个分类问题 Classification Problem**。

🔴 **分类**是指：我们设法预测一个离散值输出，0 或 1 （良性 或 恶性），有时你也有两个以上的可能的输出值，因此，你可能要设法预测离散值0、1、2...。即**分类的目的就是预测离散值输出**

在分类问题中，有另一种方法来绘制这些数据，如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200525224714.png" style="zoom:50%;" />

在这个例子中，我们只使用了一个特征或者说属性，即肿瘤的大小，来预测肿瘤是恶性的还是良性的。

在其他的机器学习问题中，我们会有多个特征、多个属性，如下例所示：

💬 假设我们不仅知道肿瘤的大小，还知道病人的年纪，我们所拥有的数据集用下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200525225228.png" style="zoom:50%;" />

学习算法所需要做的就是在数据上拟合出一条直线将恶性肿瘤和良性肿瘤分隔开来，如下图所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200525225345.png" style="zoom:50%;" />

显然，在现实生活中，我们所需要处理的特征远不止一个两个。对于某些学习问题来说，我们想要的不是使用三个还是五个特征，而是无穷多的特征、无穷多的属性，因此我们的学习算法需要使用很多的属性或特征或线索来做预测，那么如何来处理无穷多的特征呢？如何在计算机中存储无穷多数量的事物呢？

### ④ 每节小问

❓ <u>基于本节的学习，提出一个问题：</u>

判断下面两种情况属于分类问题还是回归问题：

- **Problem1**：You have a large inventory of identical items. You want to predict how many of these items will sell over the next 3 months. 你有很多同一件货物的库存，假设你有几千件相同的货物要卖，你想预测在接下来的三个月内，你能卖出多少件。

  👉 Regression Problem 回归问题：把要卖的货物数量看成一个连续的值

- **Problem2**：You'd like software to examine individual customer accounts, adn for each account decide if it has been hacked/compromise. 你有很多用户，你想要写一个软件，来检查每一个客户的账户，判断这个账户是否被入侵或破坏

  👉  Classification Problem 分类问题：设置预测的值为 0 表示账户没有被入侵，1 表示被入侵

---



## 3. 无监督学习 Unsupervised Learning

### ① 定义

在监督学习中，每个样本都被标明为（labled）阳性样本或者阴性样本，我们已被清除的告知了什么是正确答案（即肿瘤是良性还是恶性）。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526101702.png" style="zoom:50%;" />

而**对于无监督学习来说，我们的数据集没有任何标签**，或者说都具有相同的标签或者没有标签，即我们不知道什么是正确答案。我们拿到这个数据集，不知道要拿它做什么，也不知道每个数据点究竟是什么，我们只被告知这里有一个数据集，你能在其中找到某种结构吗？

<img src="https://gitee.com/veal98/images/raw/master/img/20200526101925.png" style="zoom:50%;" />

### ② 聚类算法 Clustering 

对于给定的数据集，无监督学习算法可能将该数据集分成两个不同的簇，这就是**聚类算法 clustering algorithm**

<img src="https://gitee.com/veal98/images/raw/master/img/20200526102330.png" style="zoom:50%;" />

💬 一个应用聚类算法的典型例子就是谷歌新闻，谷歌新闻会搜索成千上万的新闻，然后自动的对它们进行分簇，有关同一主题的新闻被分在同一类，比如关于新冠病毒的新闻放在同一类。

起始聚类算法和无监督学习算法也可以被用于许多其他的问题，这里我们举个它在基因组学中的应用：

💬 下图是一个 DNA 微阵列数据，基本思想是给定一组不同的个体，对于每个个体，检测它们是否拥有某个特定的基因，也就是要检测，特定基因的表达程度。这些颜色红、绿、灰等展示了不同的个体拥有特定基因的程度。然后我们所能做的就是运行一个聚类算法，把不同的个体，归入不同的类，或归入不同类型的人。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526103404.png" style="zoom:50%;" />

💬 聚类算法还被广泛应用于如下场合：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526104043.png" style="zoom: 50%;" />

聚类只是无监督学习的一种，现在我们来介绍下一种：鸡尾酒会算法

### ③ 鸡尾酒会算法 Cocktail party

首先介绍以下鸡尾酒会问题 Cocktail party problem：有一个宴会，一屋子的人，因为有许多人在同时说话，有许多声音混杂在一起，你几乎很难听清你面前的人说的话。假设一个鸡尾酒会只有两个人，同时说话，我们将两个麦克风放在房间里，且这两个麦克风与两个人的距离不同，每个麦克风记录了来自两人声音的不同组合。我们能做的就是把这两个录音交给一种无监督学习算法，称为 "**鸡尾酒会算法 cocktail party algorithm**"，让算法帮你找出数据的结构，该算法就会**分离出这两个被混叠在一起的声音**。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526105754.png" style="zoom:50%;" />

鸡尾酒会算法的 Matlab 实现：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526110514.png" style="zoom: 40%;" />

### ④ 每节小问

Of the following examples, which would you address using an <u>unsupervised learning</u> algorithm? 判断下面哪些是无监督学习算法：

- ❌ Given email labeled as spam/not spam, learn a spam filter. 垃圾邮件
- ✅ Given a set of news articles found on the web, group them into set of articles about the same story. 新闻归类
- ✅  Given a database of customer data, automatically discover market segments and group customers into different market segments. 市场划分
- ❌ Given a database of patients diagnosed as either having diabetes (糖尿病) or not, learn to classify new patients as having diabetes or not. 糖尿病预测



---



# 二、单变量线性回归 Linear Regression with One Variable

## 1. 模型表示

在监督学习里，我们有一个数据集，他被称为训练集，

✍ 我们需要使用一些符号：

- m：表示训练样本的数量 Number of training examples
- x：输入变量 / 特征 input variable / features
- y：输出变量 / 目标变量 output variable / target variable
- (x，y)：一个训练样本 one training example
- ($x^(i)$，$y^(i)$)：第 i 个训练样本
- h 代表学习算法的解决方案或函数也称为假设（**hypothesis**）

<img src="https://gitee.com/veal98/images/raw/master/img/20200526114539.png" style="zoom: 60%;" />

OK，如何给训练集下定义呢？我们先来了解监督学习算法是怎样工作的：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526115120.png" style="zoom: 67%;" />

我们怎么表示我们的**假设函数 hypothesis**呢？这里我们选择最初的假设函数作为我们接下来的函数：

🔢 $h_θ(x) = θ_0 + θ_1*x$

如果我们已有数据集输入数据 x 和输出数据 y，那么假设函数的作用就是预测 y 是关于 x 的线性函数

<img src="https://gitee.com/veal98/images/raw/master/img/20200526120027.png" style="zoom: 67%;" />

🔴 这种模型也被称为**线性回归 linear regression，即拟合出一条直线最佳匹配我们所拥有的数据**。上面这个例子是一元线性回归，即单变量 x 的函数，也称**单变量 one variable 线性回归**

## 2. 代价函数 Cost function

对于假设函数：$h_θ(x) = θ_0 + θ_1*x$

θi 我们称之为模型参数，我们接下来要讲解的就是如何选择两个参数值 $θ_0$ 和 $θ_1$ 

我们会得到不同的假设，不同的假设函数

<img src="https://gitee.com/veal98/images/raw/master/img/20200526142215.png" style="zoom: 50%;" />

我们所要做的，就是求出参数值 $θ_0$ 和 $θ_1$ ，来让假设函数表示的直线最大程度的拟合我们的数据

🔴 我们给出标准的定义：在线性回归中，我们要解决的就是一个**最小化问题 minimization**，希望找到参数值 $θ_0$ 和 $θ_1$ 使得 $h_θ(x) - y$ 尽可能的小，那么我们需要做的事情就是**尽量减少假设的输出与实际输出之间的差的平方**。

我们对所有样本进行求和，即对 i = 1 到 i = m (训练集的样本容量) 的样本，将对假设得到的预测值和实际值所得的差的平方进行相加得到总和。函数如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526143754.png" style="zoom:67%;" />

1/2m 是为了使得我们的数据更加直观，**minimize θ0θ1 指关于 $θ_0$ 和 $θ_1$ 的最小化问题**，即找到参数值 $θ_0$ 和 $θ_1$ 使得所有 $h_θ(x) - y$ 的平方和的 1/2m 尽可能的小。这便是线性回归的整体目标函数。

为了让它更明确一点，我们来改写这个函数：

🚩 定义一个**代价函数**：$J(θ_0,θ_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^(i)) - y^(i))^2$

即 $minimize_{θ0θ1} J(θ_0,θ_1)$

**我们的目标便是选择出可以使得误差的平方和能够最小的模型参数，即使得代价函数最小。**

我们绘制一个等高线图，三个坐标分别为 $θ_0$ 和 $θ_1$ 和 $J(θ_0,θ_1)$：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526203123.png" style="zoom:67%;" />

则可以看出在三维空间中存在一个使得 $J(θ_0,θ_1)$ 最小的点。

$J(θ_0,θ_1)$ 就是代价函数 cost function，也称为**平方误差函数 squared error function**，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

也许这个函数 $J(θ_0,θ_1)$ 有点抽象，可能你仍然不知道它的内涵，在接下来的几个视频里，我们要更进一步解释代价函数 J 的工作原理，并更直观地解释它在计算什么，以及我们使用它的目的。

## 3. 代价函数的直观理解 Ⅰ 

在上一节中，我们给出了代价函数一个数学上的定义。在这个视频里，让我们通过一些例子来获取一些直观的感受，看看代价函数到底是在干什么。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526203852.png" style="zoom: 67%;" />

为了更好地使代价函数 J 可视化，我们使用一个简化的假设函数：$h_θ(x) = θ_1*x$，代价函数和之前相同。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526204201.png" style="zoom: 67%;" />

基于上述简化的假设函数，我们来解释**假设函数和代价函数之间的关系**：

- 对于假设函数 h：它是关于 x 的函数

- 对于代价函数 J：它是关于 $θ_1$ 的函数

每一个代价函数都对应一个假设函数，每一个不同的 θ1 ，我们都可以得到一个不同的 J(θ1) 的值

<img src="https://gitee.com/veal98/images/raw/master/img/20200526213110.png" style="zoom: 67%;" />

🎯 **学习算法的优化目标，就是通过选择 θ1 的值，获得最小的 J(θ1)** 。在上面的曲线中，当 θ1 = 1时，J(θ1) 最小，$h_{θ1}(x)$ 也确实是最佳匹配所有数据的直线。

下面我们将讨论带两个参数 θ0 和 θ1 时的图形

## 4. 代价函数的直观理解 Ⅱ 

我们保留全部参数 θ0 和 θ1 进行讨论：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526214546.png" style="zoom: 50%;" />

我们的代价函数图形最终会如下所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200526214915.png" style="zoom:50%;" />

为了使这个图形显得直观，我们使用等高线图 contour plots 或者称为等高图像 contour figures 来表示，则可以看出在三维空间中存在一个使得 $J(θ_0,θ_1)$  最小的点。

![](https://gitee.com/veal98/images/raw/master/img/20200526220039.png)

通过这些图形，我希望你能更好地理解这些代价函数 J 所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数 J 的最小值。

**当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数取最小值的参数和来。**

我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的 θ0 和 θ1 的值，在下一节中，我们将介绍一种算法，能够自动地找出能使代价函数 J 最小化的参数 θ0 和 θ1 的值。

## 5. 梯度下降 Gradient descent

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数 $J(θ_0,θ_1)$ 的最小值。

💡 梯度下降的思想 **Outline**：

- Start with some θ0，θ1  开始时我们随机选择一个参数的组合（θ0,θ1......θn），计算代价函数

- Keep changing  θ0，θ1  to reduce $J(θ_0,θ_1)$ 

  until we hopefully end up at a minimum

  然后我们不断地改变参数组合的值，使得 J 的值减小。我们持续这么做直到到到一个**局部最小值（local minimum）**，因为我们并没有尝试完所有的参数组合，所以**不能确定**我们得到的局部最小值是否便是**全局最小值（global minimum）**，选择不同的初始参数组合，可能会找到不同的局部最小值。

![](https://gitee.com/veal98/images/raw/master/img/20200526222712.png)

💭 想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转 360 度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。

🔴 **批量梯度下降（batch gradient descent）**算法的公式为：

![](https://gitee.com/veal98/images/raw/master/img/20200526222841.png)

> `:=` 这个符号表示赋值，这是一个赋值运算符 assignment operator
>
> 比如：a := b 表示将 b 的值赋给 a。
>
> 对于 `=` ，a = b 这是一个真假判定 truth assertion

其中 **α** 是**学习率（learning rate）**，它决定了梯度下降时迈出的步子有多大。如果 α 很大，那么梯度下降就很迅速。在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们要更新 θ0 和 θ1，当 j = 0和 j = 1 时，会产生更新，所以你将更新 J(θ0) 和 J(θ1)。实现梯度下降算法的微妙之处是，在这个表达式中，如果你要更新这个等式，你需要**同时更新 simultaneously update θ0 和 θ1**，我的意思是在这个等式中，我们要这样更新：

🚩 **θ0 := θ0 减去某项 ，并同时更新 θ1 := θ1 减去某项**

实现方法是：应该先计算公式右边的部分并存储起来，通过那一部分计算出 θ0 和 θ1 的值，同时更新  θ0 和 θ1。

<img src="https://gitee.com/veal98/images/raw/master/img/20200526224446.png"  />

❌ 错误的做法：非同步更新，如果不同步更新，那么temp1 使用的 θ0 就是已经更新过的 θ0

<img src="https://gitee.com/veal98/images/raw/master/img/20200526224530.png" style="zoom:67%;" />

同步更新是更自然的实现方法。<u>当人们谈到梯度下降时，他们的意思就是同步更新</u>。

在接下来的视频中，我们要进入批量梯度下降公式中的微分项 $α\frac{\partial}{\partial θ_j}J(θ_0,θ_1)$ 的细节之中

## 6. 梯度下降的直观理解

## 7. 梯度下降的线性回归

## 8. 接下来的内容



---



# 三、线性代数回顾(Linear Algebra Review)

---

## 1. 矩阵和向量
## 2. 加法和标量乘法
## 3. 矩阵向量乘法

## 4. 矩阵乘法

## 5. 矩阵乘法的性质

## 6. 逆、转置



---



# 📚 References

- 🤖 [吴恩达机器学习经典名课【中英字幕】](https://www.bilibili.com/video/BV164411S78V?p=2)
- 🧀 [黄海广- deeplearning_ai_books](https://github.com/fengdu78/deeplearning_ai_books)
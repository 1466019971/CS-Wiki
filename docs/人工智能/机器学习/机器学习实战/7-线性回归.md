# 🌐 线性回归

---

> 🚀 从本节开始介绍回归方法，回归和前面章节介绍的分类方法同属于监督学习方法。

我们前边提到的分类的目标变量是标称型数据，而回归则是对连续型的数据做出处理，回归的目的是预测数值型数据的目标值。

## 1. 用线性回归找到最佳拟合直线

### ① 线性回归的概念

回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。

假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:

<u>HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio</u>

这就是所谓的 **回归方程(regression equation)**，其中的 0.0015 和 -0.99 称作 **回归系数（regression weights）**，求这些回归系数的过程就是回归。一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是**用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值**。我们这里所说的，回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。

⭐ 说到回归，一般都是指 **线性回归(linear regression)**。<u>线性回归意味着可以将输入项分别乘以一些常量，再将结果加起来得到输出</u>。

> 📜 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。

### ② 线性回归工作原理

❓ 我们应该怎样从一大堆数据里求出回归方程呢？ 

假定输入数据存放在矩阵 X 中，而回归系数存放在向量 w 中。那么对于给定的数据 X1，预测结果将会通过 $Y = X_1^T w$ 给出。现在的问题是，手里有一些 X 和对应的 y，怎样才能找到 w 呢？

一个常用的方法就是找出使误差最小的 `w` 。这里的误差是指预测 y 值和真实 y 值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用**平方误差函数**（实际上就是我们通常所说的**最小二乘法**）。

平方误差可以写做（**代价/损失函数**）:

<img src="https://gitee.com/veal98/images/raw/master/img/20200719140033.png" style="zoom:88%;" />

用矩阵表示还可以写做 <img src="https://gitee.com/veal98/images/raw/master/img/20200719140117.png" style="zoom:80%;" />

如果对 w 求导，得到 <img src="https://gitee.com/veal98/images/raw/master/img/20200719140216.png" style="zoom:80%;" />，令其等于零，解出 w 如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200719140238.png" style="zoom:80%;" />

> w 上方的小标记表示这是当前可以估计出的 w 的最优解。

有上述公式可知，需要对矩阵求逆，因此**这个方程只在矩阵可逆的时候适用**，我们在程序代码中对此作出判断。 判断矩阵是否可逆的一个可选方案是：

📜 判断矩阵的行列式是否为 0，若为 0 ，矩阵不可逆，不为 0 的话，矩阵可逆。

⭐ 由此总结出**线性回归的工作原理**：

- 读入数据，将数据特征 X、特征标签 y 存储在矩阵 X、y 中
- 验证 $X^TX$ 矩阵是否可逆
- 使用最小二乘法求得回归系数 w 的最佳估计

### ③ 线性回归的工作流程

- 收集数据: 采用任意方法收集数据
- 准备数据: 回归需要数值型数据，标称型数据将被转换成二值型数据
- 分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
- 训练算法: 找到回归系数
- 测试算法: 使用 $R^2$ 或者预测值和数据的拟合度，来分析模型的效果
- 使用算法: 使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签

### ④ 实际案例

根据下图中的点，找出该数据的最佳拟合直线。

![](https://gitee.com/veal98/images/raw/master/img/20200719141147.png)

数据格式如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200719141218.png" style="zoom:80%;" />

第一个特征值总是等于 1.0 即 $x_0$

✍ 数据导入函数：

```python
def loadDataSet(fileName):
    numFeat = len(open(fileName).readline().split('\t')) - 1 # 获取每行字段的个数 3
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t') # 当前行数据
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[-1]))
    return dataMat,labelMat
```

✍ 线性回归函数：

```python
import numpy as np

def standRegres(xArr, yArr):
    """
    Desc:
        求出最佳 w
    Args：
        xArrr：特征向量
        yArr： 标签向量
    Return:
    	返回回归系数 w
    """
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    xTx = xMat.T*xMat
    if np.linalg.det(xTx) == 0.0: # 计算行列式
        print("特征矩阵不可逆！")
        return
    ws = xTx.I * (xMat.T * yMat)
    return ws
```

🏃‍ 运行结果：

![](https://gitee.com/veal98/images/raw/master/img/20200719144733.png)

⭐ 则我们的最佳拟合直线为：$y_{Hat} = ws[0]*x_0 + ws[1]*x_1$

```python
xMat = np.mat(xArr)
yMat = np.mat(yArr)
ws = standRegres(xArr,yArr)
yHat = xMat*ws
```

![](https://gitee.com/veal98/images/raw/master/img/20200719145556.png)

OK，现在就可以绘制最佳直线图：

```python
def regression1():
    xArr, yArr = loadDataSet("ex0.txt")
    xMat = np.mat(xArr)
    yMat = np.mat(yArr)
    ws = standRegres(xArr, yArr)
    fig = plt.figure()
    ax = fig.add_subplot(111)  #add_subplot(349)函数的参数的意思是，将画布分成3行4列图像画在从左到右从上到下第9块
    ax.scatter(xMat[:, 1].flatten().A[0], yMat.T[:, 0].flatten().A[0]) #scatter 的x是xMat中的第二列，y是yMat的第一列
    xCopy = xMat.copy() 
    xCopy.sort(0)
    yHat = xCopy * ws
    ax.plot(xCopy[:, 1], yHat)
    plt.show()
```

> 💡：`xMat.flatten().A[0]` 解析：
>
> `xMat` 是个矩阵， `xMat.flatten()` 就是把 `xMat` 降到一维，默认是按横的方向降。降维后 `xMat` 还是个矩阵，`矩阵.A`（等效于 `矩阵.getA()`）将矩阵转换成数组，`A[0]` 就是数组里的第一个元素
>
> ![](https://gitee.com/veal98/images/raw/master/img/20200719151232.png)
>
> ![](https://gitee.com/veal98/images/raw/master/img/20200719151349.png)
>
> ![](https://gitee.com/veal98/images/raw/master/img/20200719151546.png)

🏃‍ 代码运行结果如下：

![](https://gitee.com/veal98/images/raw/master/img/20200719151614.png)

## 2. 局部加权线性回归 Locally Weighted Linear Regression, LWLR

线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。

其中一个方法是**局部加权线性回归（Locally Weighted Linear Regression，LWLR）**。⭐ 它的核心思想是：**在做预测时，更多地参考距离预测样本近的已知样本，而更少地参考距离预测样本远的已知样本。**

在这个算法中，我们给预测点附近的每个点赋予一定的权重，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。

与 kNN 一样，这种算法每次预测均需要事先选出对应的数据子集。⭐ 该算法解出回归系数 w 的形式如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200719154120.png" style="zoom:80%;" />

> 🚨 注意：式中的 W 是权重矩阵，用来给每个数据点赋予权重，而 $\hat w$ 是回归系数。两者不要搞混了！

📜 权重矩阵 W 的求解如下：

LWLR 使用 核 来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核。$w^{(i)}$ 表示每一个样本的权重：

<img src="https://gitee.com/veal98/images/raw/master/img/20200719154237.png" style="zoom:80%;" />

🚩 这样就构建了一个只含对角元素的权重矩阵 **W**，并且点 `x(i)`（表示特定的需预测样本） 与 `x`（表示已知样本 — 数据集中的每个点） 越近，该点 $x^{(i)}$ 的权重 `w(i)` 将会越大。已知样本与预测样本的距离越远，该已知样本的权重就会越小。

随着已知样本与预测样本距离的增加，权重将以指数级衰减，输入参数 `k` 控制衰减的速度，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系：

<img src="https://gitee.com/veal98/images/raw/master/img/20200719155113.png" style="zoom:80%;" />

从上图可知，**`k` 越大权重减小的速率越慢，`k` 越小权重减小的速率越快**

> 💡 我觉得可以这样理解：`k` 越大表示每个待测点用来训练的数据点越多。`k` 越小表示每个待测点用来训练的数据点越少。

Python 实现局部加权线性回归函数：

```python
def lwlr(testPoint, xArr, yArr, k = 1.0):
    """
    Desc:
        局部加权线性回归函数,单点估计
    Args:
        testPoint: 需预测的数据点
        xArr：特征矩阵
        yArr：标签矩阵
        k: 手动设置参数 k
    """
    xMat = np.mat(xArr)
    yMat = np.mat(yArr).T
    m = np.shape(xMat)[0] # m 是样本个数
    weights =  np.mat(np.eye((m))) # 创建一个对角矩阵用来保存权重w
    for j in range(m):
        diffMat = testPoint - xMat[j,:] # 预测样本 - 已知样本
        weights[j,j] = np.exp(diffMat*diffMat.T/(-2.0*k**2))
    xTx = xMat.T *(weights * xMat)
    if np.linalg.det(xTx) == 0.0:
        print("矩阵不可逆！")
        return
    ws = xTx.I * (xMat.T * weights * yMat) # 回归系数
    return testPoint * ws # 返回加权线性回归后的数据点预测值

def lwlr(testArr, xArr, yArr, k = 1.0):
    """
    Desc:
        利用  lwlr 进行预测，数据集所有点的估计
        给定特征集，输出预测集 yHat
    """
    m = np.shape(testArr)[0]
    yHat = np.zeros(m)
    for i in range(m):
        yHat[i] = lwlr(testArr[i], xArr, yArr, k)
    return yHat # 返回所有数据点加权线性回归后的预测值
```

预测某个点的标签值：

![](https://gitee.com/veal98/images/raw/master/img/20200719210804.png)

预测数据集所有点的标签值：

![](https://gitee.com/veal98/images/raw/master/img/20200719210836.png)

接下来可视化这条最佳拟合直线：

```python
#test for LWLR
def regression2():
    xArr, yArr = loadDataSet("ex0.txt")
    yHat = lwlrTest(xArr, xArr, yArr, 0.003)
    xMat = np.mat(xArr)
    srtInd = xMat[:,1].argsort(0) # argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出
    xSort=xMat[srtInd][:,0,:] # 排序
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(xSort[:,1], yHat[srtInd]) # 最佳拟合直线
    ax.scatter(xMat[:,1].flatten().A[0], np.mat(yArr).T.flatten().A[0] , s=2, c='red') # 原始数据集
    plt.show()
```

![](https://gitee.com/veal98/images/raw/master/img/20200719211947.png)

如同可知，当 k = 1.0 时，如同将所有的数据视为等权重，得出的最佳拟合直线和标准的线性回归一致。

当 k = 0.01 时，效果最好

当 k = 0.03 时，发生了过拟合。

## 3. 示例：预测鲍鱼的年龄

## 4. 缩减系数来理解数据

### ① 岭回归

### ② lasso

### ③ 前向逐步回归

## 5. 权衡偏差与方差

## 6. 示例：预测乐高玩具套装的价格

## 📚 References

- 《Machine Learning in Action》
- 《统计学习方法 - 李航》
- 《机器学习 - 周志华》
- [机器学习-线性回归和局部加权线性回归](https://www.cnblogs.com/jiading/p/11701420.html)
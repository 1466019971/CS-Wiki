# 🌴 树回归

---

上一章介绍的线性回归方法创建模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂的时候，构建全局模型显然十分困难。而且，实际问题很多都是非线性的，不可能使用全局线性模型来拟合任何数据。

一种可行的方法就是将数据集切分成很多份容易建模的数据，然后利用上一章线性回归的技术来建模。如果首次切分后仍然难以拟合线性模型，就继续切分。

## 1. 树构建算法比较

我们在 第2章 决策树中使用的树构建算法是 **ID3** 。**ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分**。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，🚨① **所以有观点认为这种切分方式过于迅速**。

除了切分过于迅速外， 🚨② **ID3 算法还存在另一个问题，它不能直接处理连续型特征**。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。

⭐ 另外一种方法是**二元切分法**，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树（<u>解决 ID3 问题 1</u>）。使用**二元切分法**易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树（<u>解决 ID3 问题 2</u>）。

> 💡 另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。

⚪ **CART (Classification And Regression Trees, 分类回归树) ** 是十分著名且广泛记载的树构建算法，**它使用二元切分来处理连续型变量**。相比于ID3算法，显然CART算法更有优势，因为我们知道<u>**CART算法不仅可以用于分类，还可以用于回归**</u>，这里的回归即是处理连续型特征的体现。

回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型

## 2. 连续和离散型特征的树的构建

在树的构建过程中，需要解决多种类型数据的存储问题。与第 2 章决策树类似，这里将使用一部字典来存储树的数据结构，该字典包含以下 4 个元素：

- 待切分的特征
- 待切分的特征值
- 右子树。当数据不能再切分时，也可以是叶节点
- 左子树。当数据不能再切分时，也可以是叶节点

这与第 2 章决策树有点不同，第 2 章中用一部字典存储所有切分，可以包含两个及以上的值。而 CART 算法只做二元切分，所以这里可以固定树的数据结构。

本章我们将构建两种树：

- 第一种是**回归树 regression tree**，其每个叶节点包含单个值
- 第二种是**模型树 model tree**，其每个叶节点包含一个线性方程

🔺 树构建算法，函数 `createTree() `伪代码大致如下:

```python
找到最佳的待切分特征:
    如果该节点不能再分，将该节点存为叶节点
    执行二元切分
    在右子树调用 createTree() 方法
    在左子树调用 createTree() 方法
```

Python 具体实现如下：

```python
import numpy as np

def loadDataSet(fileName):
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = list(map(float,curLine)) # 将每行的内容保存成一组浮点数
        dataMat.append(fltLine)
    return dataMat

def binSplitDataSet(dataSet,feature,value):
    """
    Desc:
        根据第feature列的value值进行切分，该函数通过数组过滤方式将数据集切分成两个子集并返回
    Args:
        dataSet: 数据集
        feature: 待切分的特征
        value: 切分该特征的阈值
    """
    # nonzero(dataSet[:, feature] > value)  返回结果为true行的index下标
    mat0 = dataSet[np.nonzero(dataSet[:,feature] > value)[0], :]
    mat1 = dataSet[np.nonzero(dataSet[:,feature] <= value)[0], :]
    return mat0, mat1

def createTree(dataSet,leafType=regLeaf, errType=regErr, ops=(1,4)):
    """
    Desc:
        树构建函数(递归函数)
    Args:
        dataSet：数据集
        leafType：建立叶节点的函数
        errType：误差计算函数
        ops：包含树构建所需其他参数的元组
    """
    feat, val = chooseBestSplit(dataSet, leafType, errType, ops) # 将数据集分成两个部分
    if feat == None:  # 终止条件
        return val 
    # 如果不满足停止条件，继续递归调用 createTree 函数
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    lSet, rSet = binSplitDataSet(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree  
```

`createTree `中很多函数暂未实现，后面会陆续实现，我们先看前两个函数：

```python
testMat = np.mat(np.eye(4))
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200722220728.png" style="zoom:80%;" />

这样就创建了一个简单的矩阵，现在按照指定列的某个值来切分该矩阵：

```python
mat0,mat1=binSplitDataSet(testMat,1,0.5)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200722220819.png" style="zoom:80%;" />

OK，下面将给出回归树的 `chooseBestSplit` 函数 👇

## 3. 回归树

**回归树假设叶节点是常数值**，认为数据中的复杂关系可以用树结构来概括。	

🔺 误差计算准则：**平方误差的总值**（总方差）：首先计算所有数据的均值，然后计算每条数据的值到均值的差值，为了正负差值同等对待，一般使用绝对值或平方值来代替上述差值。

### ① 构建树

接下来我们补充 `createTree` 中缺失的函数：`chooseBestSplit`。即给定某个误差计算方法，该函数会找到数据集上最佳的二元切分方式。另外，该函数还要明确什么时候停止切分，一旦停止切分就会生成一个叶节点。因此：`chooseBestSplit` 函数所需要做的两件事如下：

- 用最佳方式切分数据集
- 生成相应的叶节点

其中，`chooseBestSplit`  函数拥有的三个参数：

- `leafType`：建立叶节点的函数
- `errType`：误差计算函数
- `ops`：包含树构建所需其他参数的元组

📄 `chooseBestSplit`   的伪代码如下：

```python
对每个特征:
    对每个特征值: 
        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）
        计算切分的误差
        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差
返回最佳切分的特征和阈值
```

✍ Python 实现如下：

```python
def regLeaf(dataSet): # 建立叶节点的函数
    return np.mean(dataSet[:,-1])

def regErr(dataSet): # 误差计算函数
    # np.var 均方差函数
    return np.var(dataSet[:,-1]) * np.shape(dataSet)[0] # 均方差 * 样本个数

def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    """chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)

    Args:
        dataSet   加载的原始数据集
        leafType  建立叶子点的函数
        errType   误差计算函数(求总方差)
        ops       [容许误差下降值，切分的最少样本数]。
    Returns:
        bestIndex 最佳切分特征的下标
        bestValue 切分的最优值
    """
    tolS = ops[0]  # 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分
    tolN = ops[1] # 切分的最小样本数，小于这个大小，就不继续划分了
    
    # 如果结果集最后一列为1个变量，就返回退出
    # .T 对数据集进行转置
    # .tolist()[0] 转化为数组并取第0列
    if len(set(dataSet[:,-1].T.tolist()[0])) == 1: # 终止条件 1
        return None, leafType(dataSet)
    
    m,n = np.shape(dataSet) # 计算行列值
    S = errType(dataSet) # 无分类误差的总方差和
    bestS = np.inf
    bestIndex = 0
    bestValue = 0
    # 循环处理每一列对应的feature值 
    for featIndex in range(n-1):   # 对于每个特征
        # [0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]
        for splitVal in set(dataSet[:,featIndex].T.tolist()[0]): 
            # 对该列进行二元切分
            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) 
            # 判断二元切分的方式的元素数量是否符合预期
            if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN): 
                continue
            # 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差
            # 如果划分后误差小于 bestS，则说明找到了新的bestS
            newS = errType(mat0) + errType(mat1)
            if newS < bestS: 
                bestIndex = featIndex
                bestValue = splitVal
                bestS = newS
   # 判断二元切分的方式的元素误差是否符合预期
    if (S - bestS) < tolS:  # 终止条件 2
        return None, leafType(dataSet)
    
    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)
    
    # 对整体的成员进行判断，是否符合预期
    # 如果集合的 size 小于 tolN 
    if (np.shape(mat0)[0] < tolN) or (np.shape(mat1)[0] < tolN):   # 终止条件 3
        return None, leafType(dataSet)
    
    return bestIndex,bestValue
```

 从上述代码中，我们不难看出，在选取最佳切分特征和特征值过程中，有三种情况不会对数据集进行切分，而是直接创建叶节点。

- 如果数据集切分之前，该数据集样本所有的目标变量值相同，那么不需要切分数据集，而直接将目标变量值作为叶节点返回

- 当切分数据集后，误差的减小程度不够大（小于 `tolS`）,就不需要切分，而是直接求取数据集目标变量的均值作为叶节点值返回

- 当数据集切分后如果某个子集的样本个数小于 `tolN`，也不需要切分，而直接生成叶节点

### ② 运行代码

首先，我们看看在简单数据集上构建的回归树。我们的数据集如下所示：

<img src="https://gitee.com/veal98/images/raw/master/img/20200722224437.png" style="zoom:67%;" />

```python
myDat = loadDataSet('ex00.txt')
myMat = np.mat(myDat)
createTree(myMat)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200722230310.png" style="zoom:80%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20200722230329.png" style="zoom:80%;" />

再看一个复杂的数据集：

<img src="https://gitee.com/veal98/images/raw/master/img/20200722230542.png" style="zoom: 67%;" />

```python
myDat1 = loadDataSet('ex0.txt')
myMat1 = np.mat(myDat1)
createTree(myMat1)
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200722230652.png" style="zoom:80%;" />

<img src="https://gitee.com/veal98/images/raw/master/img/20200722230349.png" style="zoom:80%;" />

## 4.  树剪枝

### ① 预剪枝

### ② 后剪枝

## 5. 模型树

## 6. 示例 ：树回归与标准回归的比较

## 📚 References

- 《Machine Learning in Action》

- 《统计学习方法 - 李航》

- 《机器学习 - 周志华》

- [机器学习实战之树回归](https://www.cnblogs.com/zy230530/p/6985230.html)

  